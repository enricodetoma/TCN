{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/sruan2/TCN/blob/master/sequential_mnist.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BOMRnzSa7rk"
   },
   "source": [
    "# Simple RNN Models for Sequential MNIST with Tensorflow\n",
    "\n",
    "Based on the work of [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples/) and [Sungjoon](https://github.com/sjchoi86/tensorflow-101/blob/master/notebooks/rnn_mnist_simple.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNP2vmzaa7rm"
   },
   "source": [
    "## RNN Overview\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" alt=\"nn\" style=\"width: 600px;\"/>\n",
    "\n",
    "References:\n",
    "- [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf), Sepp Hochreiter & Jurgen Schmidhuber, Neural Computation 9(8): 1735-1780, 1997.\n",
    "\n",
    "## MNIST Dataset Overview\n",
    "\n",
    "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "To classify images using a recurrent neural network, we consider every image row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then handle 28 sequences of 28 timesteps for every sample.\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avKi_M3hdW7G"
   },
   "source": [
    "## System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "469NP3vAdc9U",
    "outputId": "400c2f17-74ba-43d5-ccec-2f2f6692a63a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watermark in ./tcn-env/lib/python3.6/site-packages (1.6.0)\n",
      "Requirement already satisfied: ipython in ./tcn-env/lib/python3.6/site-packages (from watermark) (6.4.0)\n",
      "Requirement already satisfied: jedi>=0.10 in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (0.12.0)\n",
      "Requirement already satisfied: pygments in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (2.2.0)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (0.1.0)\n",
      "Requirement already satisfied: simplegeneric>0.8 in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (0.8.1)\n",
      "Requirement already satisfied: decorator in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (4.3.0)\n",
      "Requirement already satisfied: pickleshare in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (0.7.4)\n",
      "Requirement already satisfied: traitlets>=4.2 in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (4.3.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (1.0.15)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (4.5.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (39.1.0)\n",
      "Requirement already satisfied: backcall in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (0.1.0)\n",
      "Requirement already satisfied: parso>=0.2.0 in ./tcn-env/lib/python3.6/site-packages (from jedi>=0.10->ipython->watermark) (0.2.0)\n",
      "Requirement already satisfied: ipython-genutils in ./tcn-env/lib/python3.6/site-packages (from traitlets>=4.2->ipython->watermark) (0.2.0)\n",
      "Requirement already satisfied: six in ./tcn-env/lib/python3.6/site-packages (from traitlets>=4.2->ipython->watermark) (1.11.0)\n",
      "Requirement already satisfied: wcwidth in ./tcn-env/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.15->ipython->watermark) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./tcn-env/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython->watermark) (0.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "A-fpqklBefZy",
    "outputId": "c04ded85-334d-4e69-b64d-3c977f050e13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherryruan/github/cs231n/cs231n-venv/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING:tensorflow:From <ipython-input-1-24428371c31d>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "WARNING:tensorflow:From /Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./TCN/mnist_pixel/data/mnist/raw/train-images-idx3-ubyte.gz\n",
      "Extracting"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "WARNING:tensorflow:From /Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ./TCN/mnist_pixel/data/mnist/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./TCN/mnist_pixel/data/mnist/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./TCN/mnist_pixel/data/mnist/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random \n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./TCN/mnist_pixel/data/mnist/raw\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "-xqyUfB-hGzC",
    "outputId": "c8eebe6a-61be-4143-8dcc-93cd00c76929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.3\n",
      "IPython 4.0.1\n",
      "\n",
      "tensorflow 1.8.0\n",
      "numpy 1.14.3\n",
      "\n",
      "compiler   : GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.37)\n",
      "system     : Darwin\n",
      "release    : 16.7.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "Git hash   : 00e4561211bf03416c7914b7c7a60219b71160de\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -m -p tensorflow,numpy -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Of-jSp2TddTS"
   },
   "source": [
    "## Row-by-row Sequential MNIST\n",
    "\n",
    "How RNN model for row-by-row sequential MNIST works from [Sungjoon's notebook](https://github.com/sjchoi86/tensorflow-101/blob/master/notebooks/rnn_mnist_simple.ipynb):\n",
    "![](https://raw.githubusercontent.com/sjchoi86/Tensorflow-101/582cc1d946f0ecbce078e493b8ccb1d7b40684df/notebooks/images/etc/rnn_mnist_look.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiiU7wPsi7Im"
   },
   "source": [
    "### Basic LSTM Model\n",
    "Directly taken from [Aymeric Damien's notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCtnM4cdjRPg"
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 1e-3\n",
    "training_steps = 10000\n",
    "batch_size = 64\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28*28 # timesteps\n",
    "num_hidden = 130 # hidden layer num of features as in TCN paper\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cr8TEextjVFG"
   },
   "outputs": [],
   "source": [
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BnMayzeIjZ4a"
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "xSgImScnjaYe",
    "outputId": "e9e3bda9-bb2b-41f3-8254-b08a504f4f87"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-a2579e9588c7>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = RNN(X, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Change optimizer to RMSProp and add gradient crop\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "gvs = optimizer.compute_gradients(loss_op)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "train_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69950\n"
     ]
    }
   ],
   "source": [
    "# count total number of parameters\n",
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    #print(shape)\n",
    "    #print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        #print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    #print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69950"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way to count total number of parameters\n",
    "np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "B8Alb6-Ljgsc",
    "outputId": "108fb22b-80c8-4368-b861-4fc7f668f882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 2.7015, Training Accuracy= 0.1562, Testing Accuracy= 0.1028\n",
      "Step 100, Minibatch Loss= 2.2935, Training Accuracy= 0.1406, Testing Accuracy= 0.0958\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-281e11c3ef47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Calculate batch loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# format test data\n",
    "test_data = mnist.test.images.reshape((-1, timesteps, num_input))\n",
    "test_label = mnist.test.labels\n",
    "f = open('test_acc_basic_lstm_sequential_mnist_784x1.txt', 'w')\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})\n",
    "            test_acc = sess.run(accuracy, feed_dict={X: test_data,Y: test_label})\n",
    "                    \n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.4f}\".format(acc)+ \\\n",
    "                  \", Testing Accuracy= \" + \"{:.4f}\".format(test_acc))\n",
    "            \n",
    "            f.write(\"%d, %f\\n\" % (step, test_acc))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "#     test_len = 128\n",
    "#     test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "#     test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KHcEqoHijfsm"
   },
   "source": [
    "### Tidier Version of the LSTM Model\n",
    "\n",
    "1. Use LSTMBlockCell, which should be faster than BasicLSTMCell\n",
    "2. Replace manual weight definitions with tf.layers.Dense\n",
    "3. Replace tf.nn.softmax_cross_entropy_with_logits with tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "4. Group graph definition together\n",
    "5. Replace rnn.dynamic_rnn with rnn.static_rnn. (So no need to unstack the tensor.)\n",
    "6. Add a batch_normalization layer between LSTM and Dense layers.\n",
    "7. Add gradient clipping for RNN gradient\n",
    "8. Add a checkpoint saver\n",
    "9. Evaluate test accuracy every N steps (BAD PRACTICE: use a validation set instead)\n",
    "10. Replace GradientDescentOptimizer with RMSPropOptimizer\n",
    "11. Use tf.set_random_seed to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPwqdhkEa7ry"
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.02\n",
    "training_steps = 5000\n",
    "batch_size = 32\n",
    "display_step = 250\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 64 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GyGSqEoqa7r2"
   },
   "outputs": [],
   "source": [
    "def RNN(x):\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.LSTMBlockCell(\n",
    "        num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    # outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "    outputs, states = tf.nn.dynamic_rnn(\n",
    "        cell=lstm_cell, inputs=x, time_major=False, dtype=tf.float32)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(\n",
    "        num_classes, activation=None, \n",
    "        kernel_initializer=tf.orthogonal_initializer()\n",
    "    )\n",
    "    return output_layer(tf.layers.batch_normalization(outputs[:, -1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "wpQ_5H6Za7r6",
    "outputId": "eeb2ecc1-c133-45bf-f6e9-d61949491124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0\n",
      "dense/bias:0\n",
      "All parameters: 73886\n",
      "Trainable parameters: 24586\n"
     ]
    }
   ],
   "source": [
    "# Need to clear the default graph before moving forward\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(1)\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    logits = RNN(X)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "    gvs = optimizer.compute_gradients(loss_op)\n",
    "    capped_gvs = [\n",
    "        (tf.clip_by_norm(grad, 2.), var) if not var.name.startswith(\"dense\") else (grad, var)\n",
    "        for grad, var in gvs]\n",
    "    for _, var in gvs:\n",
    "        if var.name.startswith(\"dense\"):\n",
    "            print(var.name)    \n",
    "    train_op = optimizer.apply_gradients(capped_gvs)  \n",
    "\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    print(\"All parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))\n",
    "    print(\"Trainable parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "juyI0OmIa7r-",
    "outputId": "9bcd73bc-82e5-4b1a-e0a7-ead32c8758df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 2.2985, Training Accuracy= 0.156, Test Accuracy= 0.047\n",
      "Step 250, Minibatch Loss= 0.6138, Training Accuracy= 0.812, Test Accuracy= 0.750\n",
      "Step 500, Minibatch Loss= 0.0504, Training Accuracy= 1.000, Test Accuracy= 0.906\n",
      "Model saved in path: /tmp/model.ckpt-500\n",
      "Step 750, Minibatch Loss= 0.0368, Training Accuracy= 1.000, Test Accuracy= 0.945\n",
      "Model saved in path: /tmp/model.ckpt-750\n",
      "Step 1000, Minibatch Loss= 0.0363, Training Accuracy= 1.000, Test Accuracy= 0.977\n",
      "Model saved in path: /tmp/model.ckpt-1000\n",
      "Step 1250, Minibatch Loss= 0.0192, Training Accuracy= 1.000, Test Accuracy= 0.953\n",
      "Step 1500, Minibatch Loss= 0.0104, Training Accuracy= 1.000, Test Accuracy= 0.938\n",
      "Step 1750, Minibatch Loss= 0.0275, Training Accuracy= 1.000, Test Accuracy= 0.977\n",
      "Step 2000, Minibatch Loss= 0.0080, Training Accuracy= 1.000, Test Accuracy= 0.969\n",
      "Step 2250, Minibatch Loss= 0.0180, Training Accuracy= 1.000, Test Accuracy= 0.969\n",
      "Step 2500, Minibatch Loss= 0.0082, Training Accuracy= 1.000, Test Accuracy= 0.945\n",
      "Step 2750, Minibatch Loss= 0.0044, Training Accuracy= 1.000, Test Accuracy= 0.977\n",
      "Step 3000, Minibatch Loss= 0.0032, Training Accuracy= 1.000, Test Accuracy= 0.961\n",
      "Step 3250, Minibatch Loss= 0.0466, Training Accuracy= 0.969, Test Accuracy= 0.953\n",
      "Step 3500, Minibatch Loss= 0.0759, Training Accuracy= 0.969, Test Accuracy= 0.977\n",
      "Step 3750, Minibatch Loss= 0.0506, Training Accuracy= 1.000, Test Accuracy= 0.992\n",
      "Model saved in path: /tmp/model.ckpt-3750\n",
      "Step 4000, Minibatch Loss= 0.0260, Training Accuracy= 1.000, Test Accuracy= 0.961\n",
      "Step 4250, Minibatch Loss= 0.2061, Training Accuracy= 0.969, Test Accuracy= 0.977\n",
      "Step 4500, Minibatch Loss= 0.0542, Training Accuracy= 1.000, Test Accuracy= 0.969\n",
      "Step 4750, Minibatch Loss= 0.2304, Training Accuracy= 0.906, Test Accuracy= 0.953\n",
      "Step 5000, Minibatch Loss= 0.0133, Training Accuracy= 1.000, Test Accuracy= 1.000\n",
      "Model saved in path: /tmp/model.ckpt-5000\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "best_val_acc = 0.8\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            # Calculate accuracy for 128 mnist test images\n",
    "            test_len = 128\n",
    "            test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "            test_label = mnist.test.labels[:test_len]\n",
    "            val_acc = sess.run(accuracy, feed_dict={X: test_data, Y: test_label})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc) + \", Test Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(val_acc))\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                save_path = saver.save(sess, \"/tmp/model.ckpt\", global_step=step)\n",
    "                print(\"Model saved in path: %s\" % save_path)\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3RLjSpqrs-I"
   },
   "source": [
    "## Pixel-by-pixel Sequential MNIST\n",
    "\n",
    "View every example as a 784 x 1 sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3gPee075_4o"
   },
   "source": [
    "### CudnnGRU Model\n",
    "\n",
    "Cudnn's implementation of GRU is much faster, but does not support variational dropout. It also does not support CPU mode.\n",
    "\n",
    "Also some new additions:\n",
    "\n",
    "1. Use tf.summary to save logs for Tensorboard\n",
    "2. Use tf.variable_scope to group variables and operations\n",
    "3. Use AdamOptimizer instead of RMSPropOptimizer (latter has some problem with CudnnGRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rG96Bh8c2cs4",
    "outputId": "1c524ee7-6ed0-401f-9094-89fcdcba95ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches: 1718\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.002\n",
    "training_steps = 5000\n",
    "batch_size = 32\n",
    "display_step = 250\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "print(\"Total number of batches:\", total_batch)\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 * 28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6ZQmqtuuVHQ"
   },
   "outputs": [],
   "source": [
    "def RNN(x):    \n",
    "    gru = tf.contrib.cudnn_rnn.CudnnGRU(\n",
    "        1, num_hidden,\n",
    "        kernel_initializer=tf.orthogonal_initializer())\n",
    "    outputs, _ = gru(tf.transpose(x, (1, 0, 2)))    \n",
    "    output_layer = tf.layers.Dense(\n",
    "        num_classes, activation=None, \n",
    "        kernel_initializer=tf.orthogonal_initializer(),\n",
    "        trainable=True\n",
    "    )\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return output_layer(tf.layers.batch_normalization(outputs[-1, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "hPNBPWsev0LG",
    "outputId": "4a565c4a-e228-4aef-f26a-202f56ca0c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0\n",
      "dense/bias:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(10)\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    # Define weights\n",
    "    logits = RNN(X)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=Y))\n",
    "    \n",
    "    with tf.variable_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.1)\n",
    "        gvs = optimizer.compute_gradients(loss_op)\n",
    "        capped_gvs = [\n",
    "            (tf.clip_by_norm(grad, 1.), var) if not var.name.startswith(\"dense\") else (grad, var)\n",
    "            for grad, var in gvs]\n",
    "        for _, var in gvs:\n",
    "            if var.name.startswith(\"dense\"):\n",
    "                print(var.name)\n",
    "        train_op = optimizer.apply_gradients(capped_gvs)  \n",
    "    \n",
    "    with tf.variable_scope('summarize_gradients'):\n",
    "        for grad, var in gvs:\n",
    "            norm = tf.norm(tf.clip_by_norm(grad, 10.), ord=2)\n",
    "            tf.summary.histogram(var.name.replace(\":\", \"_\") + '/gradient_l2', \n",
    "                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))\n",
    "        for grad, var in capped_gvs:\n",
    "            norm = tf.norm(grad, ord=2)\n",
    "            tf.summary.histogram(var.name.replace(\":\", \"_\") + '/gradient_clipped_l2', \n",
    "                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))\n",
    "\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.variable_scope('evaluate'):\n",
    "      # Evaluate model (with test logits, for dropout to be disabled)\n",
    "      correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "      accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "9y9WD82ezgvY",
    "outputId": "af347bfd-2b99-4f36-c393-f26983c3cc65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 2.2955, Training Accuracy= 0.156, Test Accuracy= 0.117\n",
      "Step 250, Minibatch Loss= 2.2108, Training Accuracy= 0.125, Test Accuracy= 0.117\n",
      "Step 500, Minibatch Loss= 0.8783, Training Accuracy= 0.781, Test Accuracy= 0.641\n",
      "Step 750, Minibatch Loss= 0.4712, Training Accuracy= 0.906, Test Accuracy= 0.734\n",
      "Step 1000, Minibatch Loss= 0.2107, Training Accuracy= 0.969, Test Accuracy= 0.812\n",
      "Model saved in path: /tmp/model.ckpt-1000\n",
      "Step 1250, Minibatch Loss= 0.4126, Training Accuracy= 0.875, Test Accuracy= 0.883\n",
      "Model saved in path: /tmp/model.ckpt-1250\n",
      "Step 1500, Minibatch Loss= 0.1344, Training Accuracy= 0.969, Test Accuracy= 0.859\n",
      "Step 1750, Minibatch Loss= 0.2449, Training Accuracy= 0.875, Test Accuracy= 0.898\n",
      "Model saved in path: /tmp/model.ckpt-1750\n",
      "Step 2000, Minibatch Loss= 0.1204, Training Accuracy= 0.969, Test Accuracy= 0.938\n",
      "Model saved in path: /tmp/model.ckpt-2000\n",
      "Step 2250, Minibatch Loss= 0.1673, Training Accuracy= 0.938, Test Accuracy= 0.930\n",
      "Step 2500, Minibatch Loss= 0.0659, Training Accuracy= 1.000, Test Accuracy= 0.945\n",
      "Model saved in path: /tmp/model.ckpt-2500\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "best_val_acc = 0.8\n",
    "log_dir = \"logs/cudnngru/%s\" % datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "tb_writer = tf.summary.FileWriter(log_dir, graph)\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc, summary = sess.run(\n",
    "                [loss_op, accuracy, merged_summary_op], \n",
    "                feed_dict={X: batch_x, Y: batch_y})\n",
    "            tb_writer.add_summary(summary, global_step=step)\n",
    "            tb_writer.flush()\n",
    "            # Calculate accuracy for 128 mnist test images\n",
    "            test_len = 128\n",
    "            test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "            test_label = mnist.test.labels[:test_len]\n",
    "            val_acc = sess.run(accuracy, feed_dict={X: test_data, Y: test_label})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc) + \", Test Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(val_acc))\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                save_path = saver.save(sess, \"/tmp/model.ckpt\", global_step=step)\n",
    "                print(\"Model saved in path: %s\" % save_path)\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7CephtZ6U3e"
   },
   "source": [
    "## Permuted Pixel-by-pixel Sequential MNIST\n",
    "\n",
    "Increase the difficulty by shuffling the order of the sequence (by applying the same reindexing operation for all sequences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hYUKsz2C6jiU"
   },
   "source": [
    "### CudnnGRU Model\n",
    "Basically the same. Just added a permutation operation in the graph definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "FL7UQ_u96x0-",
    "outputId": "9ba3e285-3c4c-4787-deab-550926b4aff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0\n",
      "dense/bias:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(10)\n",
    "    # tf Graph input\n",
    "    X_ = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    \n",
    "    # Permute the time steps\n",
    "    np.random.seed(100)\n",
    "    permute = np.random.permutation(784)\n",
    "    X = tf.gather(X_, permute, axis=1)   \n",
    "    \n",
    "    # Define weights\n",
    "    logits = RNN(X)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=Y))\n",
    "    \n",
    "    with tf.variable_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.1)\n",
    "        gvs = optimizer.compute_gradients(loss_op)\n",
    "        capped_gvs = [\n",
    "            (tf.clip_by_norm(grad, 1.), var) if not var.name.startswith(\"dense\") else (grad, var)\n",
    "            for grad, var in gvs]\n",
    "        for _, var in gvs:\n",
    "            if var.name.startswith(\"dense\"):\n",
    "                print(var.name)\n",
    "        train_op = optimizer.apply_gradients(capped_gvs)  \n",
    "    \n",
    "    with tf.variable_scope('summarize_gradients'):\n",
    "        for grad, var in gvs:\n",
    "            norm = tf.norm(tf.clip_by_norm(grad, 10.), ord=2)\n",
    "            tf.summary.histogram(var.name.replace(\":\", \"_\") + '/gradient_l2', \n",
    "                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))\n",
    "        for grad, var in capped_gvs:\n",
    "            norm = tf.norm(grad, ord=2)\n",
    "            tf.summary.histogram(var.name.replace(\":\", \"_\") + '/gradient_clipped_l2', \n",
    "                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))\n",
    "\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.variable_scope('evaluate'):\n",
    "      # Evaluate model (with test logits, for dropout to be disabled)\n",
    "      correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "      accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "nOa_0AN17Kqy",
    "outputId": "9c0a3e3e-fac8-4898-8533-f5295423b8d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 2.2866, Training Accuracy= 0.188, Test Accuracy= 0.078\n",
      "Step 250, Minibatch Loss= 1.5686, Training Accuracy= 0.500, Test Accuracy= 0.508\n",
      "Step 500, Minibatch Loss= 0.8842, Training Accuracy= 0.781, Test Accuracy= 0.555\n",
      "Step 750, Minibatch Loss= 0.9858, Training Accuracy= 0.719, Test Accuracy= 0.648\n",
      "Step 1000, Minibatch Loss= 0.9501, Training Accuracy= 0.750, Test Accuracy= 0.648\n",
      "Step 1250, Minibatch Loss= 0.7780, Training Accuracy= 0.688, Test Accuracy= 0.688\n",
      "Step 1500, Minibatch Loss= 1.0259, Training Accuracy= 0.688, Test Accuracy= 0.680\n",
      "Step 1750, Minibatch Loss= 0.5693, Training Accuracy= 0.906, Test Accuracy= 0.781\n",
      "Step 2000, Minibatch Loss= 0.6409, Training Accuracy= 0.844, Test Accuracy= 0.805\n",
      "Model saved in path: /tmp/model.ckpt-2000\n",
      "Step 2250, Minibatch Loss= 0.6085, Training Accuracy= 0.812, Test Accuracy= 0.719\n",
      "Step 2500, Minibatch Loss= 0.5675, Training Accuracy= 0.812, Test Accuracy= 0.750\n",
      "Step 2750, Minibatch Loss= 0.4034, Training Accuracy= 0.938, Test Accuracy= 0.781\n",
      "Step 3000, Minibatch Loss= 0.5835, Training Accuracy= 0.812, Test Accuracy= 0.797\n",
      "Step 3250, Minibatch Loss= 0.7464, Training Accuracy= 0.719, Test Accuracy= 0.789\n",
      "Step 3500, Minibatch Loss= 0.5364, Training Accuracy= 0.844, Test Accuracy= 0.805\n",
      "Step 3750, Minibatch Loss= 0.3257, Training Accuracy= 0.906, Test Accuracy= 0.812\n",
      "Model saved in path: /tmp/model.ckpt-3750\n",
      "Step 4000, Minibatch Loss= 0.4941, Training Accuracy= 0.875, Test Accuracy= 0.789\n",
      "Step 4250, Minibatch Loss= 0.4540, Training Accuracy= 0.906, Test Accuracy= 0.836\n",
      "Model saved in path: /tmp/model.ckpt-4250\n",
      "Step 4500, Minibatch Loss= 0.3693, Training Accuracy= 0.938, Test Accuracy= 0.805\n",
      "Step 4750, Minibatch Loss= 0.4006, Training Accuracy= 0.875, Test Accuracy= 0.797\n",
      "Step 5000, Minibatch Loss= 0.5489, Training Accuracy= 0.844, Test Accuracy= 0.820\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "#@title Default title text\n",
    "# Start training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "best_val_acc = 0.8\n",
    "log_dir = \"logs/cudnngru/%s\" % datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "tb_writer = tf.summary.FileWriter(log_dir, graph)\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X_: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc, summary = sess.run(\n",
    "                [loss_op, accuracy, merged_summary_op], \n",
    "                feed_dict={X_: batch_x, Y: batch_y})\n",
    "            tb_writer.add_summary(summary, global_step=step)\n",
    "            tb_writer.flush()\n",
    "            # Calculate accuracy for 128 mnist test images\n",
    "            test_len = 128\n",
    "            test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "            test_label = mnist.test.labels[:test_len]\n",
    "            val_acc = sess.run(accuracy, feed_dict={X_: test_data, Y: test_label})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc) + \", Test Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(val_acc))\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                save_path = saver.save(sess, \"/tmp/model.ckpt\", global_step=step)\n",
    "                print(\"Model saved in path: %s\" % save_path)\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2EBK0v37Zl6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sequential_mnist.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
