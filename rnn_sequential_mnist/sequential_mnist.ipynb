{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/sruan2/TCN/blob/master/sequential_mnist.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BOMRnzSa7rk"
   },
   "source": [
    "# Simple RNN Models for Sequential MNIST with Tensorflow\n",
    "\n",
    "Based on the work of [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples/) and [Sungjoon](https://github.com/sjchoi86/tensorflow-101/blob/master/notebooks/rnn_mnist_simple.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNP2vmzaa7rm"
   },
   "source": [
    "## RNN Overview\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" alt=\"nn\" style=\"width: 600px;\"/>\n",
    "\n",
    "References:\n",
    "- [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf), Sepp Hochreiter & Jurgen Schmidhuber, Neural Computation 9(8): 1735-1780, 1997.\n",
    "\n",
    "## MNIST Dataset Overview\n",
    "\n",
    "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "To classify images using a recurrent neural network, we consider every image row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then handle 28 sequences of 28 timesteps for every sample.\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avKi_M3hdW7G"
   },
   "source": [
    "## System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "469NP3vAdc9U",
    "outputId": "400c2f17-74ba-43d5-ccec-2f2f6692a63a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watermark in ./tcn-env/lib/python3.6/site-packages (1.6.0)\n",
      "Requirement already satisfied: ipython in ./tcn-env/lib/python3.6/site-packages (from watermark) (6.4.0)\n",
      "Requirement already satisfied: jedi>=0.10 in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (0.12.0)\n",
      "Requirement already satisfied: pygments in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (2.2.0)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (0.1.0)\n",
      "Requirement already satisfied: simplegeneric>0.8 in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (0.8.1)\n",
      "Requirement already satisfied: decorator in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (4.3.0)\n",
      "Requirement already satisfied: pickleshare in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (0.7.4)\n",
      "Requirement already satisfied: traitlets>=4.2 in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (4.3.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (1.0.15)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (4.5.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (39.1.0)\n",
      "Requirement already satisfied: backcall in ./tcn-env/lib/python3.6/site-packages (from ipython->watermark) (0.1.0)\n",
      "Requirement already satisfied: parso>=0.2.0 in ./tcn-env/lib/python3.6/site-packages (from jedi>=0.10->ipython->watermark) (0.2.0)\n",
      "Requirement already satisfied: ipython-genutils in ./tcn-env/lib/python3.6/site-packages (from traitlets>=4.2->ipython->watermark) (0.2.0)\n",
      "Requirement already satisfied: six in ./tcn-env/lib/python3.6/site-packages (from traitlets>=4.2->ipython->watermark) (1.11.0)\n",
      "Requirement already satisfied: wcwidth in ./tcn-env/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.15->ipython->watermark) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./tcn-env/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython->watermark) (0.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "A-fpqklBefZy",
    "outputId": "c04ded85-334d-4e69-b64d-3c977f050e13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherryruan/github/cs231n/cs231n-venv/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING:tensorflow:From <ipython-input-1-24428371c31d>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "WARNING:tensorflow:From /Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./TCN/mnist_pixel/data/mnist/raw/train-images-idx3-ubyte.gz\n",
      "Extracting"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "WARNING:tensorflow:From /Users/sherryruan/github/TCN/tcn-env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ./TCN/mnist_pixel/data/mnist/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./TCN/mnist_pixel/data/mnist/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./TCN/mnist_pixel/data/mnist/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random \n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./TCN/mnist_pixel/data/mnist/raw\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "-xqyUfB-hGzC",
    "outputId": "c8eebe6a-61be-4143-8dcc-93cd00c76929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.3\n",
      "IPython 4.0.1\n",
      "\n",
      "tensorflow 1.8.0\n",
      "numpy 1.14.3\n",
      "\n",
      "compiler   : GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.37)\n",
      "system     : Darwin\n",
      "release    : 16.7.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "Git hash   : 00e4561211bf03416c7914b7c7a60219b71160de\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -m -p tensorflow,numpy -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Of-jSp2TddTS"
   },
   "source": [
    "## Row-by-row Sequential MNIST\n",
    "\n",
    "How RNN model for row-by-row sequential MNIST works from [Sungjoon's notebook](https://github.com/sjchoi86/tensorflow-101/blob/master/notebooks/rnn_mnist_simple.ipynb):\n",
    "![](https://raw.githubusercontent.com/sjchoi86/Tensorflow-101/582cc1d946f0ecbce078e493b8ccb1d7b40684df/notebooks/images/etc/rnn_mnist_look.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiiU7wPsi7Im"
   },
   "source": [
    "### Basic LSTM Model\n",
    "Directly taken from [Aymeric Damien's notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCtnM4cdjRPg"
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 64 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cr8TEextjVFG"
   },
   "outputs": [],
   "source": [
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BnMayzeIjZ4a"
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "xSgImScnjaYe",
    "outputId": "e9e3bda9-bb2b-41f3-8254-b08a504f4f87"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-abee357436db>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = RNN(X, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "B8Alb6-Ljgsc",
    "outputId": "108fb22b-80c8-4368-b861-4fc7f668f882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 3.2737, Training Accuracy= 0.1094, Testing Accuracy= 0.1021\n",
      "Step 100, Minibatch Loss= 2.3788, Training Accuracy= 0.1016, Testing Accuracy= 0.1459\n",
      "Step 200, Minibatch Loss= 2.2135, Training Accuracy= 0.2344, Testing Accuracy= 0.2026\n",
      "Step 300, Minibatch Loss= 2.2079, Training Accuracy= 0.2109, Testing Accuracy= 0.2425\n",
      "Step 400, Minibatch Loss= 2.0978, Training Accuracy= 0.3828, Testing Accuracy= 0.2908\n",
      "Step 500, Minibatch Loss= 2.0698, Training Accuracy= 0.3594, Testing Accuracy= 0.3201\n",
      "Step 600, Minibatch Loss= 1.9682, Training Accuracy= 0.3516, Testing Accuracy= 0.3418\n",
      "Step 700, Minibatch Loss= 1.9702, Training Accuracy= 0.4219, Testing Accuracy= 0.3610\n",
      "Step 800, Minibatch Loss= 1.9905, Training Accuracy= 0.3203, Testing Accuracy= 0.3718\n",
      "Step 900, Minibatch Loss= 1.9044, Training Accuracy= 0.3672, Testing Accuracy= 0.3897\n",
      "Step 1000, Minibatch Loss= 1.8675, Training Accuracy= 0.4141, Testing Accuracy= 0.4073\n",
      "Step 1100, Minibatch Loss= 1.8324, Training Accuracy= 0.4141, Testing Accuracy= 0.4251\n",
      "Step 1200, Minibatch Loss= 1.7717, Training Accuracy= 0.4375, Testing Accuracy= 0.4420\n",
      "Step 1300, Minibatch Loss= 1.7951, Training Accuracy= 0.3906, Testing Accuracy= 0.4617\n",
      "Step 1400, Minibatch Loss= 1.7988, Training Accuracy= 0.4062, Testing Accuracy= 0.4671\n",
      "Step 1500, Minibatch Loss= 1.6569, Training Accuracy= 0.5078, Testing Accuracy= 0.4782\n",
      "Step 1600, Minibatch Loss= 1.6691, Training Accuracy= 0.4375, Testing Accuracy= 0.4940\n",
      "Step 1700, Minibatch Loss= 1.6601, Training Accuracy= 0.4688, Testing Accuracy= 0.5022\n",
      "Step 1800, Minibatch Loss= 1.5632, Training Accuracy= 0.4453, Testing Accuracy= 0.5072\n",
      "Step 1900, Minibatch Loss= 1.6355, Training Accuracy= 0.4453, Testing Accuracy= 0.5197\n",
      "Step 2000, Minibatch Loss= 1.5499, Training Accuracy= 0.4688, Testing Accuracy= 0.5196\n",
      "Step 2100, Minibatch Loss= 1.6290, Training Accuracy= 0.4531, Testing Accuracy= 0.5304\n",
      "Step 2200, Minibatch Loss= 1.4709, Training Accuracy= 0.4844, Testing Accuracy= 0.5345\n",
      "Step 2300, Minibatch Loss= 1.5540, Training Accuracy= 0.4688, Testing Accuracy= 0.5400\n",
      "Step 2400, Minibatch Loss= 1.2766, Training Accuracy= 0.6484, Testing Accuracy= 0.5468\n",
      "Step 2500, Minibatch Loss= 1.4073, Training Accuracy= 0.5547, Testing Accuracy= 0.5508\n",
      "Step 2600, Minibatch Loss= 1.3617, Training Accuracy= 0.6172, Testing Accuracy= 0.5577\n",
      "Step 2700, Minibatch Loss= 1.3719, Training Accuracy= 0.6016, Testing Accuracy= 0.5657\n",
      "Step 2800, Minibatch Loss= 1.3955, Training Accuracy= 0.6406, Testing Accuracy= 0.5692\n",
      "Step 2900, Minibatch Loss= 1.4434, Training Accuracy= 0.5000, Testing Accuracy= 0.5788\n",
      "Step 3000, Minibatch Loss= 1.3673, Training Accuracy= 0.5156, Testing Accuracy= 0.5824\n",
      "Step 3100, Minibatch Loss= 1.3281, Training Accuracy= 0.6172, Testing Accuracy= 0.5896\n",
      "Step 3200, Minibatch Loss= 1.3094, Training Accuracy= 0.5703, Testing Accuracy= 0.5940\n",
      "Step 3300, Minibatch Loss= 1.2867, Training Accuracy= 0.6250, Testing Accuracy= 0.5992\n",
      "Step 3400, Minibatch Loss= 1.2830, Training Accuracy= 0.6562, Testing Accuracy= 0.6068\n",
      "Step 3500, Minibatch Loss= 1.2726, Training Accuracy= 0.5781, Testing Accuracy= 0.6104\n",
      "Step 3600, Minibatch Loss= 1.2519, Training Accuracy= 0.6250, Testing Accuracy= 0.6178\n",
      "Step 3700, Minibatch Loss= 1.2397, Training Accuracy= 0.5703, Testing Accuracy= 0.6214\n",
      "Step 3800, Minibatch Loss= 1.1783, Training Accuracy= 0.6406, Testing Accuracy= 0.6296\n",
      "Step 3900, Minibatch Loss= 1.0977, Training Accuracy= 0.6875, Testing Accuracy= 0.6368\n",
      "Step 4000, Minibatch Loss= 1.2228, Training Accuracy= 0.6328, Testing Accuracy= 0.6359\n",
      "Step 4100, Minibatch Loss= 1.1679, Training Accuracy= 0.6328, Testing Accuracy= 0.6476\n",
      "Step 4200, Minibatch Loss= 1.1449, Training Accuracy= 0.5703, Testing Accuracy= 0.6514\n",
      "Step 4300, Minibatch Loss= 1.0569, Training Accuracy= 0.6797, Testing Accuracy= 0.6537\n",
      "Step 4400, Minibatch Loss= 1.2367, Training Accuracy= 0.5938, Testing Accuracy= 0.6581\n",
      "Step 4500, Minibatch Loss= 1.1738, Training Accuracy= 0.6484, Testing Accuracy= 0.6671\n",
      "Step 4600, Minibatch Loss= 1.0811, Training Accuracy= 0.6406, Testing Accuracy= 0.6715\n",
      "Step 4700, Minibatch Loss= 1.0581, Training Accuracy= 0.6719, Testing Accuracy= 0.6754\n",
      "Step 4800, Minibatch Loss= 1.0045, Training Accuracy= 0.6719, Testing Accuracy= 0.6792\n",
      "Step 4900, Minibatch Loss= 0.9956, Training Accuracy= 0.6875, Testing Accuracy= 0.6851\n",
      "Step 5000, Minibatch Loss= 0.9905, Training Accuracy= 0.7188, Testing Accuracy= 0.6895\n",
      "Step 5100, Minibatch Loss= 0.9364, Training Accuracy= 0.7109, Testing Accuracy= 0.6925\n",
      "Step 5200, Minibatch Loss= 0.8960, Training Accuracy= 0.7188, Testing Accuracy= 0.6970\n",
      "Step 5300, Minibatch Loss= 0.9595, Training Accuracy= 0.7031, Testing Accuracy= 0.7043\n",
      "Step 5400, Minibatch Loss= 0.9921, Training Accuracy= 0.6641, Testing Accuracy= 0.7053\n",
      "Step 5500, Minibatch Loss= 0.9020, Training Accuracy= 0.7578, Testing Accuracy= 0.7102\n",
      "Step 5600, Minibatch Loss= 0.8280, Training Accuracy= 0.7344, Testing Accuracy= 0.7135\n",
      "Step 5700, Minibatch Loss= 0.9067, Training Accuracy= 0.7109, Testing Accuracy= 0.7192\n",
      "Step 5800, Minibatch Loss= 0.8627, Training Accuracy= 0.7578, Testing Accuracy= 0.7211\n",
      "Step 5900, Minibatch Loss= 0.9564, Training Accuracy= 0.6562, Testing Accuracy= 0.7250\n",
      "Step 6000, Minibatch Loss= 0.8922, Training Accuracy= 0.7109, Testing Accuracy= 0.7293\n",
      "Step 6100, Minibatch Loss= 0.7637, Training Accuracy= 0.7969, Testing Accuracy= 0.7295\n",
      "Step 6200, Minibatch Loss= 0.8803, Training Accuracy= 0.7109, Testing Accuracy= 0.7342\n",
      "Step 6300, Minibatch Loss= 0.8069, Training Accuracy= 0.7734, Testing Accuracy= 0.7390\n",
      "Step 6400, Minibatch Loss= 0.8235, Training Accuracy= 0.7422, Testing Accuracy= 0.7409\n",
      "Step 6500, Minibatch Loss= 0.8279, Training Accuracy= 0.7422, Testing Accuracy= 0.7462\n",
      "Step 6600, Minibatch Loss= 0.7940, Training Accuracy= 0.7734, Testing Accuracy= 0.7480\n",
      "Step 6700, Minibatch Loss= 0.9352, Training Accuracy= 0.7031, Testing Accuracy= 0.7514\n",
      "Step 6800, Minibatch Loss= 0.8775, Training Accuracy= 0.7109, Testing Accuracy= 0.7534\n",
      "Step 6900, Minibatch Loss= 0.7308, Training Accuracy= 0.7891, Testing Accuracy= 0.7578\n",
      "Step 7000, Minibatch Loss= 0.7536, Training Accuracy= 0.7734, Testing Accuracy= 0.7604\n",
      "Step 7100, Minibatch Loss= 0.6801, Training Accuracy= 0.7969, Testing Accuracy= 0.7619\n",
      "Step 7200, Minibatch Loss= 0.7954, Training Accuracy= 0.7500, Testing Accuracy= 0.7652\n",
      "Step 7300, Minibatch Loss= 0.7028, Training Accuracy= 0.8203, Testing Accuracy= 0.7680\n",
      "Step 7400, Minibatch Loss= 0.6915, Training Accuracy= 0.7969, Testing Accuracy= 0.7722\n",
      "Step 7500, Minibatch Loss= 0.7117, Training Accuracy= 0.7500, Testing Accuracy= 0.7733\n",
      "Step 7600, Minibatch Loss= 0.7423, Training Accuracy= 0.7578, Testing Accuracy= 0.7751\n",
      "Step 7700, Minibatch Loss= 0.9436, Training Accuracy= 0.7266, Testing Accuracy= 0.7777\n",
      "Step 7800, Minibatch Loss= 0.6946, Training Accuracy= 0.8281, Testing Accuracy= 0.7829\n",
      "Step 7900, Minibatch Loss= 0.8200, Training Accuracy= 0.7969, Testing Accuracy= 0.7818\n",
      "Step 8000, Minibatch Loss= 0.6809, Training Accuracy= 0.8047, Testing Accuracy= 0.7837\n",
      "Step 8100, Minibatch Loss= 0.6799, Training Accuracy= 0.7969, Testing Accuracy= 0.7886\n",
      "Step 8200, Minibatch Loss= 0.7127, Training Accuracy= 0.7500, Testing Accuracy= 0.7915\n",
      "Step 8300, Minibatch Loss= 0.5941, Training Accuracy= 0.8281, Testing Accuracy= 0.7947\n",
      "Step 8400, Minibatch Loss= 0.6216, Training Accuracy= 0.8125, Testing Accuracy= 0.7957\n",
      "Step 8500, Minibatch Loss= 0.6745, Training Accuracy= 0.7969, Testing Accuracy= 0.7974\n",
      "Step 8600, Minibatch Loss= 0.6560, Training Accuracy= 0.8281, Testing Accuracy= 0.8004\n",
      "Step 8700, Minibatch Loss= 0.7458, Training Accuracy= 0.7734, Testing Accuracy= 0.8023\n",
      "Step 8800, Minibatch Loss= 0.5680, Training Accuracy= 0.7891, Testing Accuracy= 0.8073\n",
      "Step 8900, Minibatch Loss= 0.6096, Training Accuracy= 0.8125, Testing Accuracy= 0.8062\n",
      "Step 9000, Minibatch Loss= 0.6860, Training Accuracy= 0.7656, Testing Accuracy= 0.8095\n",
      "Step 9100, Minibatch Loss= 0.5861, Training Accuracy= 0.8359, Testing Accuracy= 0.8113\n",
      "Step 9200, Minibatch Loss= 0.6435, Training Accuracy= 0.7969, Testing Accuracy= 0.8127\n",
      "Step 9300, Minibatch Loss= 0.6630, Training Accuracy= 0.8047, Testing Accuracy= 0.8153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9400, Minibatch Loss= 0.6816, Training Accuracy= 0.7969, Testing Accuracy= 0.8177\n",
      "Step 9500, Minibatch Loss= 0.5591, Training Accuracy= 0.8359, Testing Accuracy= 0.8199\n",
      "Step 9600, Minibatch Loss= 0.4851, Training Accuracy= 0.8828, Testing Accuracy= 0.8236\n",
      "Step 9700, Minibatch Loss= 0.6727, Training Accuracy= 0.8438, Testing Accuracy= 0.8243\n",
      "Step 9800, Minibatch Loss= 0.5280, Training Accuracy= 0.8359, Testing Accuracy= 0.8267\n",
      "Step 9900, Minibatch Loss= 0.6132, Training Accuracy= 0.8203, Testing Accuracy= 0.8271\n",
      "Step 10000, Minibatch Loss= 0.6323, Training Accuracy= 0.7656, Testing Accuracy= 0.8307\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8307\n"
     ]
    }
   ],
   "source": [
    "# format test data\n",
    "test_data = mnist.test.images.reshape((-1, timesteps, num_input))\n",
    "test_label = mnist.test.labels\n",
    "f = open('test_acc_basic_lstm_sequential_mnist.txt', 'w')\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})\n",
    "            test_acc = sess.run(accuracy, feed_dict={X: test_data,Y: test_label})\n",
    "                    \n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.4f}\".format(acc)+ \\\n",
    "                  \", Testing Accuracy= \" + \"{:.4f}\".format(test_acc))\n",
    "            \n",
    "            f.write(\"%d, %f\\n\" % (step, test_acc))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "#     test_len = 128\n",
    "#     test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "#     test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KHcEqoHijfsm"
   },
   "source": [
    "### Tidier Version of the LSTM Model\n",
    "\n",
    "1. Use LSTMBlockCell, which should be faster than BasicLSTMCell\n",
    "2. Replace manual weight definitions with tf.layers.Dense\n",
    "3. Replace tf.nn.softmax_cross_entropy_with_logits with tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "4. Group graph definition together\n",
    "5. Replace rnn.dynamic_rnn with rnn.static_rnn. (So no need to unstack the tensor.)\n",
    "6. Add a batch_normalization layer between LSTM and Dense layers.\n",
    "7. Add gradient clipping for RNN gradient\n",
    "8. Add a checkpoint saver\n",
    "9. Evaluate test accuracy every N steps (BAD PRACTICE: use a validation set instead)\n",
    "10. Replace GradientDescentOptimizer with RMSPropOptimizer\n",
    "11. Use tf.set_random_seed to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPwqdhkEa7ry"
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.02\n",
    "training_steps = 5000\n",
    "batch_size = 32\n",
    "display_step = 250\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 64 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GyGSqEoqa7r2"
   },
   "outputs": [],
   "source": [
    "def RNN(x):\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.LSTMBlockCell(\n",
    "        num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    # outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "    outputs, states = tf.nn.dynamic_rnn(\n",
    "        cell=lstm_cell, inputs=x, time_major=False, dtype=tf.float32)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(\n",
    "        num_classes, activation=None, \n",
    "        kernel_initializer=tf.orthogonal_initializer()\n",
    "    )\n",
    "    return output_layer(tf.layers.batch_normalization(outputs[:, -1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "wpQ_5H6Za7r6",
    "outputId": "eeb2ecc1-c133-45bf-f6e9-d61949491124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0\n",
      "dense/bias:0\n",
      "All parameters: 73886\n",
      "Trainable parameters: 24586\n"
     ]
    }
   ],
   "source": [
    "# Need to clear the default graph before moving forward\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(1)\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    logits = RNN(X)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "    gvs = optimizer.compute_gradients(loss_op)\n",
    "    capped_gvs = [\n",
    "        (tf.clip_by_norm(grad, 2.), var) if not var.name.startswith(\"dense\") else (grad, var)\n",
    "        for grad, var in gvs]\n",
    "    for _, var in gvs:\n",
    "        if var.name.startswith(\"dense\"):\n",
    "            print(var.name)    \n",
    "    train_op = optimizer.apply_gradients(capped_gvs)  \n",
    "\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    print(\"All parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))\n",
    "    print(\"Trainable parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "juyI0OmIa7r-",
    "outputId": "9bcd73bc-82e5-4b1a-e0a7-ead32c8758df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 2.2985, Training Accuracy= 0.156, Test Accuracy= 0.047\n",
      "Step 250, Minibatch Loss= 0.6138, Training Accuracy= 0.812, Test Accuracy= 0.750\n",
      "Step 500, Minibatch Loss= 0.0504, Training Accuracy= 1.000, Test Accuracy= 0.906\n",
      "Model saved in path: /tmp/model.ckpt-500\n",
      "Step 750, Minibatch Loss= 0.0368, Training Accuracy= 1.000, Test Accuracy= 0.945\n",
      "Model saved in path: /tmp/model.ckpt-750\n",
      "Step 1000, Minibatch Loss= 0.0363, Training Accuracy= 1.000, Test Accuracy= 0.977\n",
      "Model saved in path: /tmp/model.ckpt-1000\n",
      "Step 1250, Minibatch Loss= 0.0192, Training Accuracy= 1.000, Test Accuracy= 0.953\n",
      "Step 1500, Minibatch Loss= 0.0104, Training Accuracy= 1.000, Test Accuracy= 0.938\n",
      "Step 1750, Minibatch Loss= 0.0275, Training Accuracy= 1.000, Test Accuracy= 0.977\n",
      "Step 2000, Minibatch Loss= 0.0080, Training Accuracy= 1.000, Test Accuracy= 0.969\n",
      "Step 2250, Minibatch Loss= 0.0180, Training Accuracy= 1.000, Test Accuracy= 0.969\n",
      "Step 2500, Minibatch Loss= 0.0082, Training Accuracy= 1.000, Test Accuracy= 0.945\n",
      "Step 2750, Minibatch Loss= 0.0044, Training Accuracy= 1.000, Test Accuracy= 0.977\n",
      "Step 3000, Minibatch Loss= 0.0032, Training Accuracy= 1.000, Test Accuracy= 0.961\n",
      "Step 3250, Minibatch Loss= 0.0466, Training Accuracy= 0.969, Test Accuracy= 0.953\n",
      "Step 3500, Minibatch Loss= 0.0759, Training Accuracy= 0.969, Test Accuracy= 0.977\n",
      "Step 3750, Minibatch Loss= 0.0506, Training Accuracy= 1.000, Test Accuracy= 0.992\n",
      "Model saved in path: /tmp/model.ckpt-3750\n",
      "Step 4000, Minibatch Loss= 0.0260, Training Accuracy= 1.000, Test Accuracy= 0.961\n",
      "Step 4250, Minibatch Loss= 0.2061, Training Accuracy= 0.969, Test Accuracy= 0.977\n",
      "Step 4500, Minibatch Loss= 0.0542, Training Accuracy= 1.000, Test Accuracy= 0.969\n",
      "Step 4750, Minibatch Loss= 0.2304, Training Accuracy= 0.906, Test Accuracy= 0.953\n",
      "Step 5000, Minibatch Loss= 0.0133, Training Accuracy= 1.000, Test Accuracy= 1.000\n",
      "Model saved in path: /tmp/model.ckpt-5000\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "best_val_acc = 0.8\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            # Calculate accuracy for 128 mnist test images\n",
    "            test_len = 128\n",
    "            test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "            test_label = mnist.test.labels[:test_len]\n",
    "            val_acc = sess.run(accuracy, feed_dict={X: test_data, Y: test_label})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc) + \", Test Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(val_acc))\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                save_path = saver.save(sess, \"/tmp/model.ckpt\", global_step=step)\n",
    "                print(\"Model saved in path: %s\" % save_path)\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3RLjSpqrs-I"
   },
   "source": [
    "## Pixel-by-pixel Sequential MNIST\n",
    "\n",
    "View every example as a 784 x 1 sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3gPee075_4o"
   },
   "source": [
    "### CudnnGRU Model\n",
    "\n",
    "Cudnn's implementation of GRU is much faster, but does not support variational dropout. It also does not support CPU mode.\n",
    "\n",
    "Also some new additions:\n",
    "\n",
    "1. Use tf.summary to save logs for Tensorboard\n",
    "2. Use tf.variable_scope to group variables and operations\n",
    "3. Use AdamOptimizer instead of RMSPropOptimizer (latter has some problem with CudnnGRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rG96Bh8c2cs4",
    "outputId": "1c524ee7-6ed0-401f-9094-89fcdcba95ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches: 1718\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.002\n",
    "training_steps = 5000\n",
    "batch_size = 32\n",
    "display_step = 250\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "print(\"Total number of batches:\", total_batch)\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 * 28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6ZQmqtuuVHQ"
   },
   "outputs": [],
   "source": [
    "def RNN(x):    \n",
    "    gru = tf.contrib.cudnn_rnn.CudnnGRU(\n",
    "        1, num_hidden,\n",
    "        kernel_initializer=tf.orthogonal_initializer())\n",
    "    outputs, _ = gru(tf.transpose(x, (1, 0, 2)))    \n",
    "    output_layer = tf.layers.Dense(\n",
    "        num_classes, activation=None, \n",
    "        kernel_initializer=tf.orthogonal_initializer(),\n",
    "        trainable=True\n",
    "    )\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return output_layer(tf.layers.batch_normalization(outputs[-1, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "hPNBPWsev0LG",
    "outputId": "4a565c4a-e228-4aef-f26a-202f56ca0c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0\n",
      "dense/bias:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(10)\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    # Define weights\n",
    "    logits = RNN(X)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=Y))\n",
    "    \n",
    "    with tf.variable_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.1)\n",
    "        gvs = optimizer.compute_gradients(loss_op)\n",
    "        capped_gvs = [\n",
    "            (tf.clip_by_norm(grad, 1.), var) if not var.name.startswith(\"dense\") else (grad, var)\n",
    "            for grad, var in gvs]\n",
    "        for _, var in gvs:\n",
    "            if var.name.startswith(\"dense\"):\n",
    "                print(var.name)\n",
    "        train_op = optimizer.apply_gradients(capped_gvs)  \n",
    "    \n",
    "    with tf.variable_scope('summarize_gradients'):\n",
    "        for grad, var in gvs:\n",
    "            norm = tf.norm(tf.clip_by_norm(grad, 10.), ord=2)\n",
    "            tf.summary.histogram(var.name.replace(\":\", \"_\") + '/gradient_l2', \n",
    "                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))\n",
    "        for grad, var in capped_gvs:\n",
    "            norm = tf.norm(grad, ord=2)\n",
    "            tf.summary.histogram(var.name.replace(\":\", \"_\") + '/gradient_clipped_l2', \n",
    "                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))\n",
    "\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.variable_scope('evaluate'):\n",
    "      # Evaluate model (with test logits, for dropout to be disabled)\n",
    "      correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "      accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "9y9WD82ezgvY",
    "outputId": "af347bfd-2b99-4f36-c393-f26983c3cc65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 2.2955, Training Accuracy= 0.156, Test Accuracy= 0.117\n",
      "Step 250, Minibatch Loss= 2.2108, Training Accuracy= 0.125, Test Accuracy= 0.117\n",
      "Step 500, Minibatch Loss= 0.8783, Training Accuracy= 0.781, Test Accuracy= 0.641\n",
      "Step 750, Minibatch Loss= 0.4712, Training Accuracy= 0.906, Test Accuracy= 0.734\n",
      "Step 1000, Minibatch Loss= 0.2107, Training Accuracy= 0.969, Test Accuracy= 0.812\n",
      "Model saved in path: /tmp/model.ckpt-1000\n",
      "Step 1250, Minibatch Loss= 0.4126, Training Accuracy= 0.875, Test Accuracy= 0.883\n",
      "Model saved in path: /tmp/model.ckpt-1250\n",
      "Step 1500, Minibatch Loss= 0.1344, Training Accuracy= 0.969, Test Accuracy= 0.859\n",
      "Step 1750, Minibatch Loss= 0.2449, Training Accuracy= 0.875, Test Accuracy= 0.898\n",
      "Model saved in path: /tmp/model.ckpt-1750\n",
      "Step 2000, Minibatch Loss= 0.1204, Training Accuracy= 0.969, Test Accuracy= 0.938\n",
      "Model saved in path: /tmp/model.ckpt-2000\n",
      "Step 2250, Minibatch Loss= 0.1673, Training Accuracy= 0.938, Test Accuracy= 0.930\n",
      "Step 2500, Minibatch Loss= 0.0659, Training Accuracy= 1.000, Test Accuracy= 0.945\n",
      "Model saved in path: /tmp/model.ckpt-2500\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "best_val_acc = 0.8\n",
    "log_dir = \"logs/cudnngru/%s\" % datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "tb_writer = tf.summary.FileWriter(log_dir, graph)\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc, summary = sess.run(\n",
    "                [loss_op, accuracy, merged_summary_op], \n",
    "                feed_dict={X: batch_x, Y: batch_y})\n",
    "            tb_writer.add_summary(summary, global_step=step)\n",
    "            tb_writer.flush()\n",
    "            # Calculate accuracy for 128 mnist test images\n",
    "            test_len = 128\n",
    "            test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "            test_label = mnist.test.labels[:test_len]\n",
    "            val_acc = sess.run(accuracy, feed_dict={X: test_data, Y: test_label})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc) + \", Test Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(val_acc))\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                save_path = saver.save(sess, \"/tmp/model.ckpt\", global_step=step)\n",
    "                print(\"Model saved in path: %s\" % save_path)\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7CephtZ6U3e"
   },
   "source": [
    "## Permuted Pixel-by-pixel Sequential MNIST\n",
    "\n",
    "Increase the difficulty by shuffling the order of the sequence (by applying the same reindexing operation for all sequences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hYUKsz2C6jiU"
   },
   "source": [
    "### CudnnGRU Model\n",
    "Basically the same. Just added a permutation operation in the graph definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "FL7UQ_u96x0-",
    "outputId": "9ba3e285-3c4c-4787-deab-550926b4aff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0\n",
      "dense/bias:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(10)\n",
    "    # tf Graph input\n",
    "    X_ = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    \n",
    "    # Permute the time steps\n",
    "    np.random.seed(100)\n",
    "    permute = np.random.permutation(784)\n",
    "    X = tf.gather(X_, permute, axis=1)   \n",
    "    \n",
    "    # Define weights\n",
    "    logits = RNN(X)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=Y))\n",
    "    \n",
    "    with tf.variable_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.1)\n",
    "        gvs = optimizer.compute_gradients(loss_op)\n",
    "        capped_gvs = [\n",
    "            (tf.clip_by_norm(grad, 1.), var) if not var.name.startswith(\"dense\") else (grad, var)\n",
    "            for grad, var in gvs]\n",
    "        for _, var in gvs:\n",
    "            if var.name.startswith(\"dense\"):\n",
    "                print(var.name)\n",
    "        train_op = optimizer.apply_gradients(capped_gvs)  \n",
    "    \n",
    "    with tf.variable_scope('summarize_gradients'):\n",
    "        for grad, var in gvs:\n",
    "            norm = tf.norm(tf.clip_by_norm(grad, 10.), ord=2)\n",
    "            tf.summary.histogram(var.name.replace(\":\", \"_\") + '/gradient_l2', \n",
    "                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))\n",
    "        for grad, var in capped_gvs:\n",
    "            norm = tf.norm(grad, ord=2)\n",
    "            tf.summary.histogram(var.name.replace(\":\", \"_\") + '/gradient_clipped_l2', \n",
    "                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))\n",
    "\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.variable_scope('evaluate'):\n",
    "      # Evaluate model (with test logits, for dropout to be disabled)\n",
    "      correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "      accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "nOa_0AN17Kqy",
    "outputId": "9c0a3e3e-fac8-4898-8533-f5295423b8d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 2.2866, Training Accuracy= 0.188, Test Accuracy= 0.078\n",
      "Step 250, Minibatch Loss= 1.5686, Training Accuracy= 0.500, Test Accuracy= 0.508\n",
      "Step 500, Minibatch Loss= 0.8842, Training Accuracy= 0.781, Test Accuracy= 0.555\n",
      "Step 750, Minibatch Loss= 0.9858, Training Accuracy= 0.719, Test Accuracy= 0.648\n",
      "Step 1000, Minibatch Loss= 0.9501, Training Accuracy= 0.750, Test Accuracy= 0.648\n",
      "Step 1250, Minibatch Loss= 0.7780, Training Accuracy= 0.688, Test Accuracy= 0.688\n",
      "Step 1500, Minibatch Loss= 1.0259, Training Accuracy= 0.688, Test Accuracy= 0.680\n",
      "Step 1750, Minibatch Loss= 0.5693, Training Accuracy= 0.906, Test Accuracy= 0.781\n",
      "Step 2000, Minibatch Loss= 0.6409, Training Accuracy= 0.844, Test Accuracy= 0.805\n",
      "Model saved in path: /tmp/model.ckpt-2000\n",
      "Step 2250, Minibatch Loss= 0.6085, Training Accuracy= 0.812, Test Accuracy= 0.719\n",
      "Step 2500, Minibatch Loss= 0.5675, Training Accuracy= 0.812, Test Accuracy= 0.750\n",
      "Step 2750, Minibatch Loss= 0.4034, Training Accuracy= 0.938, Test Accuracy= 0.781\n",
      "Step 3000, Minibatch Loss= 0.5835, Training Accuracy= 0.812, Test Accuracy= 0.797\n",
      "Step 3250, Minibatch Loss= 0.7464, Training Accuracy= 0.719, Test Accuracy= 0.789\n",
      "Step 3500, Minibatch Loss= 0.5364, Training Accuracy= 0.844, Test Accuracy= 0.805\n",
      "Step 3750, Minibatch Loss= 0.3257, Training Accuracy= 0.906, Test Accuracy= 0.812\n",
      "Model saved in path: /tmp/model.ckpt-3750\n",
      "Step 4000, Minibatch Loss= 0.4941, Training Accuracy= 0.875, Test Accuracy= 0.789\n",
      "Step 4250, Minibatch Loss= 0.4540, Training Accuracy= 0.906, Test Accuracy= 0.836\n",
      "Model saved in path: /tmp/model.ckpt-4250\n",
      "Step 4500, Minibatch Loss= 0.3693, Training Accuracy= 0.938, Test Accuracy= 0.805\n",
      "Step 4750, Minibatch Loss= 0.4006, Training Accuracy= 0.875, Test Accuracy= 0.797\n",
      "Step 5000, Minibatch Loss= 0.5489, Training Accuracy= 0.844, Test Accuracy= 0.820\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "#@title Default title text\n",
    "# Start training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "best_val_acc = 0.8\n",
    "log_dir = \"logs/cudnngru/%s\" % datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "tb_writer = tf.summary.FileWriter(log_dir, graph)\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X_: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc, summary = sess.run(\n",
    "                [loss_op, accuracy, merged_summary_op], \n",
    "                feed_dict={X_: batch_x, Y: batch_y})\n",
    "            tb_writer.add_summary(summary, global_step=step)\n",
    "            tb_writer.flush()\n",
    "            # Calculate accuracy for 128 mnist test images\n",
    "            test_len = 128\n",
    "            test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "            test_label = mnist.test.labels[:test_len]\n",
    "            val_acc = sess.run(accuracy, feed_dict={X_: test_data, Y: test_label})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc) + \", Test Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(val_acc))\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                save_path = saver.save(sess, \"/tmp/model.ckpt\", global_step=step)\n",
    "                print(\"Model saved in path: %s\" % save_path)\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2EBK0v37Zl6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sequential_mnist.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
